{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OTME9HwwiJX",
        "outputId": "61642059-2b10-4f88-bbd7-ed2f044633f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "# Install the NLTK (Natural Language Toolkit) library\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the NLTK library\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer models, used for tokenizing text into sentences or words\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the 'stopwords' corpus, which contains lists of common stopwords for various languages\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYZPHT1QwrgW",
        "outputId": "a4b07c8b-5a44-4fac-8bc8-dcf070dc1933"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords  # Import the stopwords module from NLTK (Natural Language Toolkit)\n",
        "from nltk.tokenize import word_tokenize  # Import the word_tokenize function from NLTK\n",
        "\n",
        "# Define a sample text string\n",
        "sample_text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "# Create a set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the sample text into words\n",
        "word_tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Initialize an empty list to store the filtered words\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:  # Iterate over each word in the tokenized words\n",
        "    if w not in stop_words:  # Check if the word is not a stopword\n",
        "        filtered_sentence.append(w)  # If not a stopword, append the word to the filtered_sentence list\n",
        "\n",
        "print(word_tokens)  # Print the list of tokenized words\n",
        "print(filtered_sentence)  # Print the list of words after stopword removal\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOF7cwc3w3iF",
        "outputId": "b5214c24-8ebd-4da9-8534-f8455baef4a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Stemming**"
      ],
      "metadata": {
        "id": "2E5oIHCtzcIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer  # Import the PorterStemmer class from NLTK for stemming words\n",
        "\n",
        "from nltk.tokenize import word_tokenize  # Import the word_tokenize function from NLT\n",
        "import time\n",
        "\n",
        "porter_stemmer = PorterStemmer()  # Create an instance of the PorterStemmer for stemming words\n",
        "\n",
        "# Sample text for stemming\n",
        "sample_text = \"\"\"The children are playing outside. They have been playing for hours,\n",
        "                running around the park. Their laughter and running echo through the trees.\n",
        "                As they play, their playful shouts and screams fill the air.\n",
        "                Even the dogs are running and playing with them.\n",
        "                Watching them play brings joy to everyone around.\"\"\"\n",
        "\n",
        "token = word_tokenize(sample_text)  # Tokenize the sample text into words\n",
        "\n",
        "# Perform stemming on each word\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in token]\n",
        "\n",
        "stemmed_text = \" \".join(stemmed_words)  # Join the stemmed words back into a single string\n",
        "\n",
        "print(f\"Original Text: {sample_text}\")  # Print the original text\n",
        "print(f\"Stemmed Text: {stemmed_text}\")  # Print the stemmed text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xehfgv_8x4yp",
        "outputId": "9aa71738-4560-468d-b21a-c1c234501353"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The children are playing outside. They have been playing for hours, \n",
            "                running around the park. Their laughter and running echo through the trees.\n",
            "                As they play, their playful shouts and screams fill the air.\n",
            "                Even the dogs are running and playing with them.\n",
            "                Watching them play brings joy to everyone around.\n",
            "Stemmed Text: the children are play outsid . they have been play for hour , run around the park . their laughter and run echo through the tree . as they play , their play shout and scream fill the air . even the dog are run and play with them . watch them play bring joy to everyon around .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Lemmatization**"
      ],
      "metadata": {
        "id": "ksBBSxOD14pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the NLTK (Natural Language Toolkit) library\n",
        "import nltk\n",
        "\n",
        "# Download the 'wordnet' dataset, which is a lexical database for the English language\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62b-1Rw5ALMn",
        "outputId": "54d79fd0-2df7-4b59-94d6-69e7f8d09895"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordNetLemmatizer class from NLTK for lemmatizing words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import the word_tokenize function from NLTK to tokenize the text\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Import the wordnet module from NLTK to provide word sense information for lemmatization\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Create an instance of the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a sample text to be lemmatized\n",
        "text = \"The children are playing outside. They have been playing for hours, running around the park.\"\n",
        "\n",
        "# Tokenize the sample text into words\n",
        "token = word_tokenize(text)\n",
        "\n",
        "# Apply lemmatization to each word in the tokenized list, using the VERB part-of-speech tag\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in token]\n",
        "\n",
        "# Join the lemmatized words back into a single string\n",
        "lemmatized_text = \" \".join(lemmatized_words)\n",
        "\n",
        "# Print the original text\n",
        "print(f\"Original Text: {text}\")\n",
        "\n",
        "# Print the lemmatized text\n",
        "print(f\"Lemmatized Text: {lemmatized_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuHROw3w1luC",
        "outputId": "17406f1a-0b77-401a-825f-4472a765d336"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The children are playing outside. They have been playing for hours, running around the park.\n",
            "Lemmatized Text: The children be play outside . They have be play for hours , run around the park .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Regex**"
      ],
      "metadata": {
        "id": "1xgUBaSzAgjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the regular expression module\n",
        "import re\n",
        "\n",
        "# Define a function to clean the text using regular expressions\n",
        "def regex_magic(text):\n",
        "    # Remove all punctuation characters from the text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Replace all sequences of digits with a space\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Define a sample text to be processed\n",
        "text = \"Alex, a 25-year-old Sub Engineer (Electrical), graduated in 2024 from the Polytech Institute. His expertise includes AC Generators, Three Phase/Star Delta Connections, and AC/DC Motors.\"\n",
        "\n",
        "# Print the original text\n",
        "print(f\"Original Text: {text}\")\n",
        "\n",
        "# Apply the regex_magic function to the text and print the cleaned text\n",
        "print(f\"Regex Magic Text: {regex_magic(text)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9egGJz9_g8j",
        "outputId": "dc916d95-d128-483a-e8ac-0609e8f3e4e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Alex, a 25-year-old Sub Engineer (Electrical), graduated in 2024 from the Polytech Institute. His expertise includes AC Generators, Three Phase/Star Delta Connections, and AC/DC Motors.\n",
            "Regex Magic Text: Alex a  yearold Sub Engineer Electrical graduated in   from the Polytech Institute His expertise includes AC Generators Three PhaseStar Delta Connections and ACDC Motors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5KJx2t7BgKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}